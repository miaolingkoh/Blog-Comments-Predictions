{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import RandomForestClassifier as RF\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import functools\n",
    "from pyspark.ml.feature import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkSession from pyspark.sql\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create my_spark or get it if already created\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blog Comments Predictions\n",
    "\n",
    "In this assignment, we will try to predict the number of comments that a blog post receives based on features of the post. \n",
    "\n",
    "Dataset is from the UCI Machine Learning Archive here: https://archive.ics.uci.edu/ml/datasets/BlogFeedback. \n",
    "It has 281 columns and we are interested in the last column for prediction, our tartget: number of comments received\n",
    "\n",
    "## Attribute Information:\n",
    "\n",
    "1...50: \n",
    "Average, standard deviation, min, max and median of the \n",
    "Attributes 51...60 for the source of the current blog post \n",
    "With source we mean the blog on which the post appeared. \n",
    "For example, myblog.blog.org would be the source of \n",
    "the post myblog.blog.org/post_2010_09_10 \n",
    "51: Total number of comments before basetime \n",
    "52: Number of comments in the last 24 hours before the \n",
    "basetime \n",
    "53: Let T1 denote the datetime 48 hours before basetime, \n",
    "Let T2 denote the datetime 24 hours before basetime. \n",
    "This attribute is the number of comments in the time period \n",
    "between T1 and T2 \n",
    "54: Number of comments in the first 24 hours after the \n",
    "publication of the blog post, but before basetime \n",
    "55: The difference of Attribute 52 and Attribute 53 \n",
    "56...60: \n",
    "The same features as the attributes 51...55, but \n",
    "features 56...60 refer to the number of links (trackbacks), \n",
    "while features 51...55 refer to the number of comments. \n",
    "61: The length of time between the publication of the blog post \n",
    "and basetime \n",
    "62: The length of the blog post \n",
    "63...262: \n",
    "The 200 bag of words features for 200 frequent words of the \n",
    "text of the blog post \n",
    "263...269: binary indicator features (0 or 1) for the weekday \n",
    "(Monday...Sunday) of the basetime \n",
    "270...276: binary indicator features (0 or 1) for the weekday \n",
    "(Monday...Sunday) of the date of publication of the blog \n",
    "post \n",
    "277: Number of parent pages: we consider a blog post P as a \n",
    "parent of blog post B, if B is a reply (trackback) to \n",
    "blog post P. \n",
    "278...280: \n",
    "Minimum, maximum, average number of comments that the \n",
    "parents received \n",
    "281: The target: the number of comments in the next 24 hours \n",
    "(relative to basetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data\n",
    "Pyspark handles only numeric data. We can be sure from the results below they are all numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading multiple csv in a directory\n",
    "df_train = spark.read.csv(['C:/Users/IP 320/Desktop/AUEB Assisgments/BlogFeedback/Train'],\\\n",
    "                   inferSchema= True)\n",
    "\n",
    "df_test = spark.read.csv(['C:/Users/IP 320/Desktop/AUEB Assisgments/BlogFeedback/Test'],\\\n",
    "                   inferSchema= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target columns and features. \n",
    "\n",
    "Here we want to understand the distribution on our target and variables.\n",
    "\n",
    "This target is the number of comments in the next 24 hours (relative to basetime). Our task to predict this number.\n",
    "\n",
    " In the histogram below, we can see that majority of our dataset have received comments in the range of 1 to 50. This means that our data is very skewed and that can means problem when testing our data as we do not have enough information to help us predict other numbers of comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Histograms below: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Comments Distribution')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Histograms below: \")\n",
    "\n",
    "comments = df_train.groupBy('_c280').count().collect()\n",
    "categories = [i[0] for i in comments]\n",
    "counts = [i[1] for i in comments]\n",
    "\n",
    "ind = np.array(range(len(categories)))\n",
    "width = 50\n",
    "plt.bar(ind, counts, width=width, color='r')\n",
    "\n",
    "plt.ylabel('counts')\n",
    "plt.title('Comments Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------+\n",
      "|   skewness(_c280)|  kurtosis(_c280)|\n",
      "+------------------+-----------------+\n",
      "|12.691311246278854|232.2954094062568|\n",
      "+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, skewness, kurtosis\n",
    "for col in df_train.columns[280:281]:\n",
    "    df_train.select(skewness(col),kurtosis(col)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that most post are posted on Wednesday and Tuesday, with least on Sunday. \n",
    "According to our research with the link below, Tuesday and Wednesday are the best days to post on social media.\n",
    "\n",
    "And lowest engagement is Sunday. Now it all makes sense.\n",
    "\n",
    "https://sproutsocial.com/insights/best-times-to-post-on-social-media/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "df_traindays = df_train.withColumnRenamed(\"_c269\", \"Mon\").\\\n",
    "        withColumnRenamed(\"_c270\", \"Tue\").\\\n",
    "        withColumnRenamed(\"_c271\", \"Wed\").\\\n",
    "        withColumnRenamed(\"_c272\", \"Thu\").\\\n",
    "        withColumnRenamed(\"_c273\", \"Fri\").\\\n",
    "        withColumnRenamed(\"_c274\", \"Sat\").\\\n",
    "        withColumnRenamed(\"_c275\", \"Sun\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date of publication of the blog post\n",
      "+--------+--------+--------+--------+--------+--------+--------+\n",
      "|sum(Mon)|sum(Tue)|sum(Wed)|sum(Thu)|sum(Fri)|sum(Sat)|sum(Sun)|\n",
      "+--------+--------+--------+--------+--------+--------+--------+\n",
      "|  8343.0|  8786.0|  8977.0|  8501.0|  8093.0|  5038.0|  4659.0|\n",
      "+--------+--------+--------+--------+--------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Date of publication of the blog post\")\n",
    "popular_dayforcomments_agg = df_traindays.agg(F.sum(df_traindays.Mon),\n",
    "                                        F.sum(df_traindays.Tue),\n",
    "                                        F.sum(df_traindays.Wed),\n",
    "                                        F.sum(df_traindays.Thu),\n",
    "                                        F.sum(df_traindays.Fri),\n",
    "                                        F.sum(df_traindays.Sat),\n",
    "                                        F.sum(df_traindays.Sun)).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation to the days of publication\n",
    "\n",
    "We want to see if there are any really any correlation between the days and number of comments they receive.\n",
    "\n",
    "I create the list of column names under num_cols and select those preselected columns from the dataframe. Using these columns, I pass it through function statistics corr. In this function, it has a parameters for method. It can include many choices and for this I am using pearson correlation. Other method such as spearman can also be used.\n",
    "\n",
    "There are some relationship between the days but not a very strong relationship. They are new friendship found. Weak.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          _c269     _c270     _c271     _c272     _c273     _c274     _c275     _c280\n",
      "_c269  1.000000 -0.195329 -0.197874 -0.191510 -0.185995 -0.141937 -0.135951 -0.000284\n",
      "_c270 -0.195329  1.000000 -0.204088 -0.197524 -0.191836 -0.146395 -0.140221  0.004695\n",
      "_c271 -0.197874 -0.204088  1.000000 -0.200098 -0.194336 -0.148303 -0.142048 -0.003085\n",
      "_c272 -0.191510 -0.197524 -0.200098  1.000000 -0.188086 -0.143533 -0.137479 -0.007672\n",
      "_c273 -0.185995 -0.191836 -0.194336 -0.188086  1.000000 -0.139399 -0.133520  0.008056\n",
      "_c274 -0.141937 -0.146395 -0.148303 -0.143533 -0.139399  1.000000 -0.101892  0.000869\n",
      "_c275 -0.135951 -0.140221 -0.142048 -0.137479 -0.133520 -0.101892  1.000000 -0.002905\n",
      "_c280 -0.000284  0.004695 -0.003085 -0.007672  0.008056  0.000869 -0.002905  1.000000\n"
     ]
    }
   ],
   "source": [
    "#correlations with the days to target variable\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import pandas as pd\n",
    "\n",
    "num_cols = ['_c269', '_c270', '_c271', '_c272', '_c273', '_c274', '_c275', '_c280']\n",
    "corr_data = df_train.select(num_cols)\n",
    "\n",
    "col_names = corr_data.columns\n",
    "features = corr_data.rdd.map(lambda row: row[0:])\n",
    "corr_mat=Statistics.corr(features, method=\"pearson\")\n",
    "corr_df = pd.DataFrame(corr_mat)\n",
    "corr_df.index, corr_df.columns = col_names, col_names\n",
    "\n",
    "print(corr_df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Linear Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare labeled set - 'features' and 'labels'\n",
    "\n",
    "I understand that the data may be skewed and we need to log the data.\n",
    "\n",
    "We will can see the r2 value and random forest gave use the better r2 score at 0.37. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create assembler for train\n",
    "label_col_train = df_train.columns[-1]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[ x for x in df_train.columns[:-1] ],\n",
    "    outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a = assembler.transform(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create assembler for test\n",
    "label_col_test = df_test.columns[-1]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[ x for x in df_test.columns[:-1] ],\n",
    "    outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_a = assembler.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(maxIter=10, regParam=0.3, labelCol= label_col_train, elasticNetParam=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "lrModel = lr.fit(train_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: 5.934188421829072\n"
     ]
    }
   ],
   "source": [
    "# Print the intercept for linear regression\n",
    "print(\"Intercept: %s\" % str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_predictions=lrModel.evaluate(train_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "918.3465108604942"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_predictions.meanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35407672011159796"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_predictions.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make predictions on test data \n",
    "test_results=lrModel.evaluate(test_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|         residuals|\n",
      "+------------------+\n",
      "|-0.395124054014091|\n",
      "|-7.671163253091358|\n",
      "| 4.689294185571059|\n",
      "| 1.749078331569856|\n",
      "|-1.793563867274801|\n",
      "| 7.789119527331314|\n",
      "|-4.881671878630961|\n",
      "| 5.348886157627678|\n",
      "|-35.13276479001333|\n",
      "| -8.89745726685041|\n",
      "+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#view the residual errors based on predictions \n",
    "test_results.residuals.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3375313998543865"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#coefficient of determination value for model\n",
    "test_results.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.82452330119175"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "616.2569571314122"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results.meanSquaredError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transData(data): return data.rdd.map(lambda r: [Vectors.dense(r[:-1]),r[-1]]).toDF(['features','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[40.30467,53.8456...|  1.0|\n",
      "|[40.30467,53.8456...|  0.0|\n",
      "|[40.30467,53.8456...|  0.0|\n",
      "|[40.30467,53.8456...|  1.0|\n",
      "|[40.30467,53.8456...| 27.0|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "transformed= transData(df_train)\n",
    "transformed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  1.0|[40.30467,53.8456...|\n",
      "|  0.0|[40.30467,53.8456...|\n",
      "|  0.0|[40.30467,53.8456...|\n",
      "|  1.0|[40.30467,53.8456...|\n",
      "| 27.0|[40.30467,53.8456...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert the data to dense vector\n",
    "def transData(data):\n",
    "    return data.rdd.map(lambda r: [r[-1], Vectors.dense(r[:-1])]).\\\n",
    "           toDF(['label','features'])\n",
    "\n",
    "transformed = transData(df_train)\n",
    "transformed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# We specify maxCategories so features with > 4\n",
    "# distinct values are treated as continuous.\n",
    "\n",
    "featureIndexer = VectorIndexer(inputCol=\"features\", \\\n",
    "                               outputCol=\"indexedFeatures\",\\\n",
    "                               maxCategories=4).fit(transformed)\n",
    "\n",
    "data = featureIndexer.transform(transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|            features|     indexedFeatures|\n",
      "+-----+--------------------+--------------------+\n",
      "|  1.0|[40.30467,53.8456...|[40.30467,53.8456...|\n",
      "|  0.0|[40.30467,53.8456...|[40.30467,53.8456...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|[6.004717,37.5324...|\n",
      "|  2.0|[7.7777777,6.1604...|\n",
      "|  0.0|[0.0,0.0,0.0,0.0,...|\n",
      "|  0.0|[34.589355,98.559...|\n",
      "|  0.0|[0.0,0.0,0.0,0.0,...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testtransformed= transData(df_test)\n",
    "testtransformed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureIndexertest = VectorIndexer(inputCol=\"features\", \\\n",
    "                               outputCol=\"indexedFeatures\",\\\n",
    "                               maxCategories=4).fit(testtransformed)\n",
    "\n",
    "test_data = featureIndexertest.transform(testtransformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|            features|     indexedFeatures|\n",
      "+-----+--------------------+--------------------+\n",
      "|  0.0|[6.004717,37.5324...|[6.004717,37.5324...|\n",
      "|  2.0|[7.7777777,6.1604...|[7.7777777,6.1604...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "dt = DecisionTreeRegressor(featuresCol=\"indexedFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain indexer and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, dt])\n",
    "\n",
    "model = pipeline.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions.\n",
    "predictions = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------------+\n",
      "|            features|label|        prediction|\n",
      "+--------------------+-----+------------------+\n",
      "|[6.004717,37.5324...|  0.0|0.5627904291857881|\n",
      "|[7.7777777,6.1604...|  2.0|0.5627904291857881|\n",
      "|[0.0,0.0,0.0,0.0,...|  0.0|0.5627904291857881|\n",
      "|[34.589355,98.559...|  0.0|0.5627904291857881|\n",
      "|[0.0,0.0,0.0,0.0,...|  0.0|0.5627904291857881|\n",
      "+--------------------+-----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select example rows to display.\n",
    "predictions.select(\"features\",\"label\",\"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 24.1084\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\",\n",
    "                                predictionCol=\"prediction\",\n",
    "                                metricName=\"rmse\")\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2_score: 0.37520240923381665\n"
     ]
    }
   ],
   "source": [
    "y_true = predictions.select(\"label\").toPandas()\n",
    "y_pred = predictions.select(\"prediction\").toPandas()\n",
    "\n",
    "import sklearn.metrics\n",
    "r2_score = sklearn.metrics.r2_score(y_true, y_pred)\n",
    "print('r2_score: {0}'.format(r2_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LinearRegression class\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "# Define LinearRegression algorithm\n",
    "rf = RandomForestRegressor(featuresCol=\"indexedFeatures\",numTrees=2, maxDepth=2, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain indexer and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, rf])\n",
    "model = pipeline.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------------+\n",
      "|            features|label|      prediction|\n",
      "+--------------------+-----+----------------+\n",
      "|[6.004717,37.5324...|  0.0|2.63584185079476|\n",
      "|[7.7777777,6.1604...|  2.0|2.63584185079476|\n",
      "|[0.0,0.0,0.0,0.0,...|  0.0|2.63584185079476|\n",
      "|[34.589355,98.559...|  0.0|2.63584185079476|\n",
      "|[0.0,0.0,0.0,0.0,...|  0.0|2.63584185079476|\n",
      "+--------------------+-----+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"features\",\"label\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 25.2131\n"
     ]
    }
   ],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2_score: 0.375\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics\n",
    "r2_score = sklearn.metrics.r2_score(y_true, y_pred)\n",
    "print('r2_score: {:4.3f}'.format(r2_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(280, {20: 0.0206, 28: 0.5967, 52: 0.1752, 54: 0.2075})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.stages[-1].featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DecisionTreeRegressionModel (uid=dtr_e39fe222df34) of depth 2 with 7 nodes,\n",
       " DecisionTreeRegressionModel (uid=dtr_6d8ca6d249cd) of depth 2 with 7 nodes]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.stages[-1].trees"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
